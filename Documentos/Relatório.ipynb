{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Inicialmente, devido ao desejo da equipe de avançar rapidamente no projeto, começamos a desenvolver modelos antes de realizar a devida análise e limpeza dos dados. Esse apressamento resultou em diversas falhas e erros ao longo do desenvolvimento. A pressa em ver resultados fez com que subestimássemos a importância de uma base de dados limpa e bem-analisada, o que nos levou a enfrentar problemas de inconsistência e ruído nos dados, prejudicando a performance dos nossos modelos.\n",
    "\n",
    "Após identificar que nossas dificuldades tinham origem na falta de uma preparação adequada dos dados, voltamos nossa atenção para a análise e limpeza dos mesmos. Realizamos uma inspeção minuciosa para identificar outliers, valores faltantes e outras anomalias. Além disso, aplicamos técnicas de normalização e transformação para garantir que os dados estivessem prontos para serem utilizados na modelagem. Esse processo, embora demorado, foi crucial para corrigir os problemas iniciais e está detalhadamente descrito em \"Análise_e_Limpeza.ipynb\".\n",
    "\n",
    "Em relação aos modelos desenvolvidos, nossa primeira tentativa foi com o K-Nearest Neighbours (KNN), devido ao caráter numérico e ordinal das features do nosso dataset. Apesar de nossas expectativas, o KNN não apresentou um bom desempenho, alcançando uma precisão de apenas cerca de 60% na plataforma Kaggle. Acreditamos que a simplicidade do modelo e a natureza dos dados tenham contribuído para esse resultado insatisfatório.\n",
    "\n",
    "Buscando melhorar os resultados, optamos por utilizar o RandomForestClassifier. Este modelo, conhecido por sua robustez e capacidade de lidar com variáveis categóricas e numéricas, também não atingiu o desempenho esperado. Embora tenha mostrado uma melhora em relação ao KNN, seus resultados ainda estavam aquém do que considerávamos aceitável para nosso projeto.\n",
    "\n",
    "Foi somente após adotarmos o XGBoostClassifier, um algoritmo de boosting bastante eficaz, que conseguimos obter resultados satisfatórios. O uso do XGBoost, combinado com uma técnica de resampling recomendada pelos organizadores da atividade, proporcionou uma melhora significativa no desempenho do modelo. Essa abordagem permitiu que lidássemos melhor com o desbalanceamento dos dados e aproveitássemos ao máximo as características do dataset, resultando em uma performance muito superior às tentativas anteriores. No caso mais bem sucedido, a taxa de aprendizado utilizada foi de 0.013.\n",
    "\n",
    "Em resumo, a experiência nos ensinou a importância de não negligenciar a etapa de preparação dos dados e a necessidade de experimentar diferentes algoritmos e técnicas até encontrar a combinação ideal para o problema em questão.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
